- scanners don't typically deal with nested constructs since, to preserve their regular expression / FA quality, nested comments are a recursive structure.
- There is no obvious one step algorithm for converting a set of regular expressions into a DFA. Rather, typical scanner generators implement the conversion as a series of three separate steps.
	1. Coversion of regular expressions to NFA
	2. Conversion of NFA into equivalent DFA
	3. Optimize DFA
- As a general rule, handwritten automata tend to use nested case statements, while most automatically generated automata use tables.
- during scanning, two aspects of the code typically deviate from the strict form of a fomral finite automaton. One is the handling of keywords. Theo ther is the need to peek ahead when a token can validly be extended by two or more additional characters, but not by only one.
- It is possible to write a finite automaton that distinguishes between keywords and identifiers, but it requires a lot of states. Most scanners, both handwritten and automattically generated, therefore treat keywords as "exceptions" to the rule for identifiers. Before returning an identifier to the parser, the scanner looks it up in a hash table or trie to make sure it isn't really a keyword.
- Whenever one legitimate token is a prefix of another, the 'longest possible token' rule says that we should continue scanning.
- In general, upcoming characters that a scanner must examine in order to make a decision are known as its look-ahead.
- Most scanners are automatically generated by programs such as lex. This alternative approach to hand-written scanners represents the automaton as a data structure: a two-dimmensional transition table. A driver program uses the current state and input character to index into the table.
- Some languages and language implementations allow a program to contain constructs called pragmas that provide directives or hints to the compiler.
