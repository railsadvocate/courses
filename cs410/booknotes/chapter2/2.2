- Together, the scanner and parser for a programming language are responsible for discovering the syntactic structure of a program
- By grouping input characters into tokens, the scanner dramatically reduces the number of individual items that must be inspected by the more computationally intensive parser. In addition, the scanner typically removes comments, saves the text of interesting tokens like identifiers and numeric literals, and tags tokens with line and column numbers to make it easier to generate high-quality error messages.
- As a rule, we accept the longest possible token in each invocation of the scanner.
- During language development, it is usually preferable to build a scanner in a more structured way, as an explicit representation of a finite automaton.
	- Finite automaton can be generated automatically from a set of regular expressions, making it easy to regenerate a scanner when token definitions change.
- scanners don't typically deal with nested constructs since, to preserve their regular expression / FA quality, nested comments are a recursive structure.
- There is no obvious one step algorithm for converting a set of regular expressions into a DFA. Rather, typical scanner generators implement the conversion as a series of three separate steps.
	1. Coversion of regular expressions to NFA
	2. Conversion of NFA into equivalent DFA
	3. Optimize DFA
- an NFA is like a DFA except that
	1. there may be more than one transition out of a given state labeled by a given character
	2. there may be so-called epsilon transitions 
- As a general rule, handwritten automata tend to use nested case statements, while most automatically generated automata use tables.
- during scanning, two aspects of the code typically deviate from the strict form of a formal finite automaton. One is the handling of keywords. The other is the need to peek ahead when a token can validly be extended by two or more additional characters, but not by only one.
- It is possible to write a finite automaton that distinguishes between keywords and identifiers, but it requires a lot of states. Most scanners, both handwritten and automattically generated, therefore treat keywords as "exceptions" to the rule for identifiers. Before returning an identifier to the parser, the scanner looks it up in a hash table or trie to make sure it isn't really a keyword.
- Whenever one legitimate token is a prefix of another, the 'longest possible token' rule says that we should continue scanning.
- In general, upcoming characters that a scanner must examine in order to make a decision are known as its look-ahead.
- Most scanners are automatically generated by programs such as lex. This alternative approach to hand-written scanners represents the automaton as a data structure: a two-dimmensional transition table. A driver program uses the current state and input character to index into the table.
- Some languages and language implementations allow a program to contain constructs called pragmas that provide directives or hints to the compiler.
- One of the chief ways that a scanner differs from a formal DFA is that is identifies tokens in addition to recognizing them. That is, it not only determines whether characters constitute a valid token; it also indicates which one.
